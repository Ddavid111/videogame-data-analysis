{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B3plUzUyAVjB",
    "outputId": "24047dae-f2bc-45c8-8c22-ef52e15fdfc3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-09 11:41:09] INFO: === Starting merge process ===\n",
      "[2025-11-09 11:41:09] INFO: Loaded: steam.csv (27075 rows)\n",
      "[2025-11-09 11:41:10] INFO: Loaded: steam_description_data_cleaned.csv (27334 rows)\n",
      "[2025-11-09 11:41:11] INFO: Loaded: steam_media_data.csv (27332 rows)\n",
      "[2025-11-09 11:41:11] INFO: Loaded: steam_support_info.csv (27136 rows)\n",
      "[2025-11-09 11:41:12] INFO: Loaded: steamspy_tag_data.csv (29022 rows)\n",
      "[2025-11-09 11:41:12] INFO: Loaded: steam_requirements_data.csv (27319 rows)\n",
      "[2025-11-09 11:41:14] INFO: SteamSpy tags converted → 28447 appid with tag data\n",
      "[2025-11-09 11:41:15] INFO: A source merged: 27075 rows\n",
      "[2025-11-09 11:41:34] INFO: B source loaded from JSON: 111452 rows\n",
      "[2025-11-09 11:41:47] INFO: B full DataFrame written to CSV\n",
      "[2025-11-09 11:41:54] INFO: Loaded: games_march2025_cleaned.csv (89618 rows)\n",
      "[2025-11-09 11:42:00] INFO: Loaded: games_march2025_full.csv (94948 rows)\n",
      "[2025-11-09 11:42:05] INFO: Loaded: games_may2024_cleaned.csv (83646 rows)\n",
      "[2025-11-09 11:42:10] INFO: Loaded: games_may2024_full.csv (87806 rows)\n",
      "[2025-11-09 11:42:10] INFO: C source combined: 356018 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Venn-diagram mentve ide: C:\\Users\\zalma\\merge\\venn_diagram.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-09 11:42:11] INFO: Merging sources with C→B→A priority...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemszámos Venn-táblázat mentve ide: C:\\Users\\zalma\\merge\\venn_table.csv\n",
      "\n",
      "=== Elemszámos Venn-diagram táblázat ===\n",
      " csak A  csak B  csak C  A ∩ B  A ∩ C  B ∩ C  A ∩ B ∩ C\n",
      "   1234    5731     161   1400      8  79888      24433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-09 11:42:32] INFO: Merge complete (112855 rows, 63 columns)\n",
      "[2025-11-09 11:42:37] INFO: Normalized thumbnail screenshots for source A (27075 items)\n",
      "[2025-11-09 11:42:37] INFO: Normalized thumbnail screenshots for source B (111452 items)\n",
      "[2025-11-09 11:42:42] INFO: Normalized thumbnail screenshots for source C (104490 items)\n",
      "[2025-11-09 11:42:44] INFO: Normalized movies for source A (25393 items)\n",
      "[2025-11-09 11:42:45] INFO: Normalized movies for source B (111452 items)\n",
      "[2025-11-09 11:42:48] INFO: Normalized movies for source C (104490 items)\n",
      "[2025-11-09 11:44:12] INFO: Combined duplicate tag columns: ['tags_x', 'tags_y'] → kept unified 'tags'\n",
      "[2025-11-09 11:44:25] INFO: Dropped redundant column: steamspy_tags\n",
      "[2025-11-09 11:44:33] INFO: Cleaned and normalized language field: supported_languages\n",
      "[2025-11-09 11:44:39] INFO: Cleaned and normalized language field: full_audio_languages\n",
      "[2025-11-09 11:44:48] INFO: Final language cleanup on supported_languages: 542206 → 542160 entries (after filtering).\n",
      "[2025-11-09 11:44:51] INFO: Final language cleanup on full_audio_languages: 212743 → 212736 entries (after filtering).\n",
      "[2025-11-09 11:44:52] INFO: 41830 rows have identical supported and audio language sets.\n",
      "[2025-11-09 11:45:07] INFO: Source summary saved: C:\\Users\\zalma\\merge\\source_summary.csv\n",
      "[2025-11-09 11:45:07] INFO: Integrity check completed, saved to C:\\Users\\zalma\\merge\\integrity_report.csv\n",
      "[2025-11-09 11:45:07] INFO: Merged master table saved to: C:\\Users\\zalma\\merge\\merged_master.csv\n",
      "[2025-11-09 11:45:07] INFO: === Merge process successfully completed ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Forrásonkénti rekordösszesítő táblázat ===\n",
      "forrás_kombináció  rekordok_száma  tartalmaz_A  tartalmaz_B  tartalmaz_C\n",
      "              C,B           79888        False         True         True\n",
      "            C,B,A           24433         True         True         True\n",
      "                B            5731        False         True        False\n",
      "              B,A            1400         True         True        False\n",
      "                A            1234         True        False        False\n",
      "                C             161        False        False         True\n",
      "              C,A               8         True        False         True\n",
      "\n",
      "=== Integritás ellenőrzési összesítő ===\n",
      "              ellenőrzés  hibák_száma\n",
      "      Duplikált appid-ek            0\n",
      "        Hiányzó appid-ek            0\n",
      "      Hiányzó játéknevek            0\n",
      "   Hiányzó forrásjelölés            0\n",
      "Érvénytelen release_date            8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "import warnings\n",
    "import sys\n",
    "import html\n",
    "\n",
    "# ======== PATHS ========\n",
    "BASE_PATH = r\"C:\\Users\\zalma\"\n",
    "A_PATH = os.path.join(BASE_PATH, \"A\")\n",
    "B_PATH = os.path.join(BASE_PATH, \"B\")\n",
    "C_PATH = os.path.join(BASE_PATH, \"C\")\n",
    "OUTPUT_PATH = os.path.join(BASE_PATH, \"merge\")\n",
    "\n",
    "# ======== LOGGING CONFIGURATION ========\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "LOG_FILE = os.path.join(OUTPUT_PATH, \"merge_log.txt\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "# ======== HELPER FUNCTIONS ========\n",
    "def load_csv_safely(path: str, **kwargs: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölt egy CSV fájlt, hiba esetén üres DataFrame-et ad vissza.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        logging.info(f\"Loaded: {os.path.basename(path)} ({len(df)} rows)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizálja a DataFrame oszlopneveit: levágja a szóközöket, kisbetűssé alakítja,\n",
    "    és helyettesíti a szóközöket és kötőjeleket alulvonással.\n",
    "    \"\"\"\n",
    "    df.columns = (\n",
    "        df.columns.str.strip().str.lower().str.replace(\" \", \"_\").str.replace(\"-\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def normalize_tags_column(val):\n",
    "    \"\"\"\n",
    "    Egységesíti a 'tags' oszlop értékeit:\n",
    "    - Ha dict-string: {'Action': 5472, 'FPS': 4897} → [{\"tag_name\": \"Action\", \"weight\": 5472}, ...]\n",
    "    - Ha már list-of-dict, érintetlenül hagyja.\n",
    "    \"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, dict):\n",
    "                return [{\"tag_name\": k, \"weight\": v} for k, v in parsed.items()]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return val\n",
    "\n",
    "\n",
    "def fill_missing_from_source(D: pd.DataFrame, src: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Kitölti a hiányzó értékeket a D DataFrame-ben egy forrás (src) adatai alapján.\n",
    "    Az appid oszlop alapján merge-öl, a közös oszlopokat balról tölti.\n",
    "    \"\"\"\n",
    "    src = src.copy()\n",
    "    src[\"appid\"] = src[\"appid\"].astype(str)\n",
    "\n",
    "    common_cols = [col for col in src.columns if col in D.columns]\n",
    "\n",
    "    merged = D.merge(\n",
    "        src[common_cols],\n",
    "        on=\"appid\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_src\")\n",
    "    )\n",
    "\n",
    "    for col in common_cols:\n",
    "        if col != \"appid\":\n",
    "            merged[col] = merged[col].combine_first(merged[f\"{col}_src\"])\n",
    "            merged.drop(columns=[f\"{col}_src\"], inplace=True)\n",
    "\n",
    "    return merged\n",
    "\n",
    "def clean_html_entities(text: str) -> str:\n",
    "    \"\"\"Eltávolítja a HTML tageket és dekódolja az entitásokat (pl. &reg; → ®).\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
    "    cleaned = soup.get_text(\" \", strip=True)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "    \n",
    "def clean_language_field_merged(val):\n",
    "    \"\"\"\n",
    "    Normalizálja a supported_languages / full_audio_languages mezőt:\n",
    "    - Tisztítja a HTML-, BBCode- és Steam-maradványokat\n",
    "    - Szétbontja az összefűzött nyelveket (, ; szóköz)\n",
    "    - Egyesíti az egyszerű HTML-tisztítást és a nyelvnév normalizálást\n",
    "    \"\"\"\n",
    "    if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "        return []\n",
    "\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        raw_items = [str(v).strip() for v in val if str(v).strip()]\n",
    "    else:\n",
    "        val_str = str(val).strip()\n",
    "        if not val_str:\n",
    "            return []\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val_str)\n",
    "            if isinstance(parsed, list):\n",
    "                raw_items = [str(v).strip() for v in parsed if v]\n",
    "            else:\n",
    "                raw_items = [val_str]\n",
    "        except Exception:\n",
    "            raw_items = [val_str]\n",
    "\n",
    "    cleaned = []\n",
    "    for x in raw_items:\n",
    "        x = html.unescape(x)\n",
    "        x = re.sub(r\"<[^>]+>\", \" \", x)\n",
    "        x = re.sub(r\"&lt;/?\\w+&gt;\", \" \", x)\n",
    "        x = re.sub(r\"&nbsp;\", \" \", x)\n",
    "        x = re.sub(r\"#lang[_\\-]?\", \"\", x, flags=re.IGNORECASE)\n",
    "        x = re.sub(r\"\\[/?[^\\]]+\\]\", \"\", x)\n",
    "        x = re.sub(r\"&[a-z]+;\", \" \", x)\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "        if x:\n",
    "            cleaned.extend(re.split(r\"[,;/]\", x))\n",
    "\n",
    "    cleaned = [c.strip() for c in cleaned if c.strip()]\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for c in cleaned:\n",
    "        key = c.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            result.append(c)\n",
    "\n",
    "    return result\n",
    "\n",
    "def final_clean_language_list(langs: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Utólagos tisztítás egy játék nyelvlistáján:\n",
    "    - HTML/BBCode/technikai morzsák eltávolítása\n",
    "    - Tipikus kettős elemek összevonása (Simplified/Traditional Chinese, Spanish – Spain/LatAm, Portuguese – Brazil/Portugal)\n",
    "    - Duplikátumok kiszűrése\n",
    "    - LAZÁBB validáció (engedjük a kötőjelet, pontot, vesszőt, számot is; 2+ karakter)\n",
    "    - Csak a nyilvánvaló szemét (lt, gt, br, strong, audio, support, text, /br) kiszűrése\n",
    "    \"\"\"\n",
    "    import re, html\n",
    "\n",
    "    if not langs or not isinstance(langs, list):\n",
    "        return []\n",
    "\n",
    "    cleaned = []\n",
    "    for name in langs:\n",
    "        if not name or not isinstance(name, str):\n",
    "            continue\n",
    "        n = html.unescape(name)\n",
    "        n = re.sub(r\"<[^>]*>\", \" \", n)\n",
    "        n = re.sub(r\"&lt;?/?\\w+&gt;?\", \" \", n)\n",
    "        n = re.sub(r\"&[a-z]+;\", \" \", n)\n",
    "        n = re.sub(r\"[\\(\\)]\", \" \", n)\n",
    "        n = re.sub(r\"#lang[_\\-]?\", \" \", n, flags=re.IGNORECASE)\n",
    "        n = re.sub(r\"\\[/?[^\\]]+\\]\", \" \", n)\n",
    "\n",
    "        n = re.sub(r\"\\b(?:lt|gt|strong|br|/br)\\b\", \" \", n, flags=re.IGNORECASE)\n",
    "\n",
    "        n = re.sub(r\"\\s+\", \" \", n).strip()\n",
    "        if not n:\n",
    "            continue\n",
    "\n",
    "        if n.isupper():\n",
    "            n = n.capitalize()\n",
    "\n",
    "        cleaned.append(n)\n",
    "\n",
    "    joined = []\n",
    "    skip = False\n",
    "    for i, word in enumerate(cleaned):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        w = word.strip()\n",
    "        nxt = cleaned[i + 1].strip().lower() if i < len(cleaned) - 1 else \"\"\n",
    "\n",
    "        def join2(a, b):\n",
    "            return f\"{a} {b}\"\n",
    "\n",
    "        low = w.lower()\n",
    "        if i < len(cleaned) - 1:\n",
    "            if low == \"simplified\" and nxt == \"chinese\":\n",
    "                joined.append(\"Simplified Chinese\"); skip = True; continue\n",
    "            if low == \"traditional\" and nxt == \"chinese\":\n",
    "                joined.append(\"Traditional Chinese\"); skip = True; continue\n",
    "            if low == \"spanish\" and \"spain\" in nxt:\n",
    "                joined.append(\"Spanish - Spain\"); skip = True; continue\n",
    "            if low == \"spanish\" and \"latin\" in nxt:\n",
    "                joined.append(\"Spanish - Latin America\"); skip = True; continue\n",
    "            if low == \"portuguese\" and \"brazil\" in nxt:\n",
    "                joined.append(\"Portuguese - Brazil\"); skip = True; continue\n",
    "            if low == \"portuguese\" and \"portugal\" in nxt:\n",
    "                joined.append(\"Portuguese - Portugal\"); skip = True; continue\n",
    "\n",
    "        joined.append(w)\n",
    "\n",
    "    standardized = []\n",
    "    for lang in joined:\n",
    "        s = lang\n",
    "        s = s.replace(\"Spanish Spain\", \"Spanish - Spain\")\n",
    "        s = s.replace(\"Spanish Latin America\", \"Spanish - Latin America\")\n",
    "        s = s.replace(\"Portuguese Brazil\", \"Portuguese - Brazil\")\n",
    "        s = s.replace(\"Portuguese Portugal\", \"Portuguese - Portugal\")\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "        standardized.append(s)\n",
    "\n",
    "    valid_lang_pattern = re.compile(r\"^[A-Za-zÀ-ÿ0-9' .,\\-]{2,}$\")\n",
    "\n",
    "    hard_exclude = re.compile(r\"\\b(?:lt|gt|br|strong|audio|support|text|/br)\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "    corrections_map = {\n",
    "        r\"\\bhe ew\\b\": \"Hebrew\",\n",
    "        r\"\\bma ese\\b\": \"Maltese\",\n",
    "        r\"\\bazil\\b\": \"Brazil\",\n",
    "        r\"\\bfran(?:c|ç)ais\\b\": \"Français\",\n",
    "    }\n",
    "\n",
    "    fixed = []\n",
    "    for lang in standardized:\n",
    "        if hard_exclude.search(lang):\n",
    "            continue\n",
    "\n",
    "        s = lang\n",
    "        for wrong_re, right in corrections_map.items():\n",
    "            s = re.sub(wrong_re, right, s, flags=re.IGNORECASE)\n",
    "\n",
    "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "        if valid_lang_pattern.match(s):\n",
    "            fixed.append(s)\n",
    "\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in fixed:\n",
    "        k = x.lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ======== SOURCE LOADING FUNCTIONS ========\n",
    "def load_source_a(a_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölti az A forrást (Steam CSV fájlok), megtisztítja az oszlopneveket,\n",
    "    és merge-eli a különböző fájlokat egy DataFrame-be.\n",
    "    \"\"\"\n",
    "    steam = load_csv_safely(os.path.join(a_path, \"steam.csv\"))\n",
    "    description = load_csv_safely(os.path.join(a_path, \"steam_description_data_cleaned.csv\"))\n",
    "    media = load_csv_safely(os.path.join(a_path, \"steam_media_data.csv\"))\n",
    "    support = load_csv_safely(os.path.join(a_path, \"steam_support_info.csv\"))\n",
    "    tags = load_csv_safely(os.path.join(a_path, \"steamspy_tag_data.csv\"))\n",
    "    reqs = load_csv_safely(os.path.join(a_path, \"steam_requirements_data.csv\"))\n",
    "\n",
    "    for df in [steam, description, media, support, tags, reqs]:\n",
    "        if not df.empty:\n",
    "            df = clean_columns(df)\n",
    "            possible_ids = [c for c in df.columns if \"appid\" in c.lower()]\n",
    "            if possible_ids:\n",
    "                df.rename(columns={possible_ids[0]: \"appid\"}, inplace=True)\n",
    "    if not tags.empty:\n",
    "        tag_cols = [c for c in tags.columns if c != \"appid\" and tags[c].dtype in [int, float]]\n",
    "        if tag_cols:\n",
    "            melted = tags.melt(\n",
    "                id_vars=[\"appid\"],\n",
    "                value_vars=tag_cols,\n",
    "                var_name=\"tag_name\",\n",
    "                value_name=\"weight\"\n",
    "            )\n",
    "            melted = melted[melted[\"weight\"] > 0]\n",
    "            tags_dict = (\n",
    "                melted.groupby(\"appid\")\n",
    "                .apply(lambda x: {t: int(w) for t, w in zip(x[\"tag_name\"], x[\"weight\"])})\n",
    "                .to_dict()\n",
    "            )\n",
    "            tags = pd.DataFrame({\"appid\": list(tags_dict.keys()), \"tags\": list(tags_dict.values())})\n",
    "            logging.info(f\"SteamSpy tags converted → {len(tags)} appid with tag data\")\n",
    "        else:\n",
    "            logging.warning(\"No numeric tag columns found in steamspy_tag_data.csv\")\n",
    "\n",
    "    merged = (\n",
    "        steam.merge(description, on=\"appid\", how=\"left\")\n",
    "        .merge(media, on=\"appid\", how=\"left\")\n",
    "        .merge(support, on=\"appid\", how=\"left\")\n",
    "        .merge(tags, on=\"appid\", how=\"left\")\n",
    "        .merge(reqs, on=\"appid\", how=\"left\")\n",
    "    )\n",
    "    logging.info(f\"A source merged: {len(merged)} rows\")\n",
    "    return merged\n",
    "\n",
    "\n",
    "def load_source_b(base_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölti a B forrást JSON fájlból, előkészíti Pandas DataFrame-re,\n",
    "    és beállítja a numerikus és logikai oszlopok típusait.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(base_path, \"games.json\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"File not found: {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "\n",
    "    records = []\n",
    "    for appID, game in dataset.items():\n",
    "        fields = [\n",
    "            \"name\",\n",
    "            \"release_date\",\n",
    "            \"estimated_owners\",\n",
    "            \"price\",\n",
    "            \"required_age\",\n",
    "            \"dlc_count\",\n",
    "            \"detailed_description\",\n",
    "            \"short_description\",\n",
    "            \"about_the_game\",\n",
    "            \"reviews\",\n",
    "            \"header_image\",\n",
    "            \"website\",\n",
    "            \"support_url\",\n",
    "            \"support_email\",\n",
    "            \"windows\",\n",
    "            \"mac\",\n",
    "            \"linux\",\n",
    "            \"metacritic_score\",\n",
    "            \"metacritic_url\",\n",
    "            \"user_score\",\n",
    "            \"positive\",\n",
    "            \"negative\",\n",
    "            \"score_rank\",\n",
    "            \"achievements\",\n",
    "            \"recommendations\",\n",
    "            \"notes\",\n",
    "            \"average_playtime_forever\",\n",
    "            \"average_playtime_2weeks\",\n",
    "            \"median_playtime_forever\",\n",
    "            \"median_playtime_2weeks\",\n",
    "            \"peak_ccu\",\n",
    "        ]\n",
    "\n",
    "        record = {key: game.get(key) for key in fields}\n",
    "        record[\"appid\"] = str(appID)\n",
    "\n",
    "        list_fields = [\n",
    "            \"packages\", \"developers\", \"publishers\", \"categories\", \"genres\",\n",
    "            \"supported_languages\", \"full_audio_languages\", \"screenshots\", \"movies\"\n",
    "        ]\n",
    "        record.update({f: game.get(f, []) for f in list_fields})\n",
    "        \n",
    "        tags = game.get(\"tags\", {})\n",
    "        record[\"tags\"] = tags if isinstance(tags, dict) else {}\n",
    "\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    df_b = pd.DataFrame(records)\n",
    "\n",
    "    if \"release_date\" in df_b.columns:\n",
    "        df_b[\"release_date\"] = pd.to_datetime(df_b[\"release_date\"], errors=\"coerce\")\n",
    "        df_b[\"release_date\"] = df_b[\"release_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df_b[\"release_date\"] = df_b[\"release_date\"].replace(\"NaT\", None)\n",
    "\n",
    "    df_b_exploded = df_b.explode(\"packages\").dropna(subset=[\"packages\"])\n",
    "\n",
    "    numeric_cols = [\n",
    "        \"metacritic_score\",\n",
    "        \"user_score\",\n",
    "        \"positive\",\n",
    "        \"negative\",\n",
    "        \"achievements\",\n",
    "        \"recommendations\",\n",
    "        \"price\",\n",
    "        \"required_age\",\n",
    "        \"dlc_count\",\n",
    "        \"average_playtime_forever\",\n",
    "        \"average_playtime_2weeks\",\n",
    "        \"median_playtime_forever\",\n",
    "        \"median_playtime_2weeks\",\n",
    "        \"peak_ccu\",\n",
    "    ]\n",
    "    \n",
    "    packages_df = pd.json_normalize(df_b.explode(\"packages\")[\"packages\"])\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df_b.columns:\n",
    "            df_b[col] = pd.to_numeric(df_b[col], errors=\"coerce\")\n",
    "\n",
    "    bool_cols = [\"windows\", \"mac\", \"linux\"]\n",
    "    for col in bool_cols:\n",
    "        if col in df_b.columns:\n",
    "            df_b[col] = df_b[col].astype(bool)\n",
    "\n",
    "    logging.info(f\"B source loaded from JSON: {len(df_b)} rows\")\n",
    "    return df_b\n",
    "\n",
    "\n",
    "def load_source_c(c_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölti a C forrást több CSV fájlból, megtisztítja az oszlopneveket,\n",
    "    és egyesíti az adatokat egy DataFrame-be.\n",
    "    \"\"\"\n",
    "    c_files = [\n",
    "        \"games_march2025_cleaned.csv\",\n",
    "        \"games_march2025_full.csv\",\n",
    "        \"games_may2024_cleaned.csv\",\n",
    "        \"games_may2024_full.csv\",\n",
    "    ]\n",
    "    c_dfs = [load_csv_safely(os.path.join(c_path, f)) for f in c_files]\n",
    "    c_dfs = [clean_columns(df) for df in c_dfs if not df.empty]\n",
    "    df_c = pd.concat(c_dfs, ignore_index=True)\n",
    "    df_c[\"appid\"] = df_c[\"appid\"].astype(str)\n",
    "    logging.info(f\"C source combined: {len(df_c)} rows\")\n",
    "    return df_c\n",
    "\n",
    "\n",
    "# ======== MERGE FUNCTION ========\n",
    "def merge_sources(a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame, columns_to_merge: list[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Összefésüli az A, B, C forrásokat AppID alapján.\n",
    "    C → B → A prioritással tölti ki a hiányzó adatokat.\n",
    "\n",
    "    Paraméter:\n",
    "        columns_to_merge: ha meg van adva, csak ezeket az oszlopokat (és appid-t) mergeli.\n",
    "    \"\"\"\n",
    "    logging.info(\"Merging sources with C→B→A priority...\")\n",
    "\n",
    "    for df in [a, b, c]:\n",
    "        if not df.empty:\n",
    "            df[\"appid\"] = df[\"appid\"].astype(str).str.strip()\n",
    "            df.drop_duplicates(subset=\"appid\", inplace=True)\n",
    "\n",
    "    if columns_to_merge:\n",
    "        keep_cols = [\"appid\"] + [col for col in columns_to_merge if col in a.columns or col in b.columns or col in c.columns]\n",
    "        a = a[[col for col in keep_cols if col in a.columns]]\n",
    "        b = b[[col for col in keep_cols if col in b.columns]]\n",
    "        c = c[[col for col in keep_cols if col in c.columns]]\n",
    "        logging.info(f\"Using subset of columns for merge: {keep_cols}\")\n",
    "\n",
    "    columns = list(dict.fromkeys(\n",
    "        sum([df.columns.tolist() for df in [a, b, c] if not df.empty], [])\n",
    "    ))\n",
    "\n",
    "    all_appids = pd.concat([a[[\"appid\"]], b[[\"appid\"]], c[[\"appid\"]]], ignore_index=True).drop_duplicates()\n",
    "\n",
    "    D = pd.DataFrame(columns=columns)\n",
    "    D[\"appid\"] = all_appids[\"appid\"]\n",
    "\n",
    "    for src in [c, b, a]:\n",
    "        if not src.empty:\n",
    "            D = fill_missing_from_source(D, src)\n",
    "\n",
    "    logging.info(f\"Merge complete ({len(D)} rows, {len(columns)} columns)\")\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "def finalize_sources(D, a, b, c):\n",
    "    \"\"\"\n",
    "    Hozzáad egy 'sources' oszlopot a D (merged_master) DataFrame-hez,\n",
    "    ami jelzi, hogy a sor melyik eredeti datasetből származik.\n",
    "    \"\"\"\n",
    "    a_ids = set(a[\"appid\"]) if not a.empty else set()\n",
    "    b_ids = set(b[\"appid\"]) if not b.empty else set()\n",
    "    c_ids = set(c[\"appid\"]) if not c.empty else set()\n",
    "\n",
    "    sources = []\n",
    "    for appid in D[\"appid\"]:\n",
    "        src = []\n",
    "        if appid in c_ids:\n",
    "            src.append(\"C\")\n",
    "        if appid in b_ids:\n",
    "            src.append(\"B\")\n",
    "        if appid in a_ids:\n",
    "            src.append(\"A\")\n",
    "        sources.append(\",\".join(src))\n",
    "\n",
    "    D[\"sources\"] = sources\n",
    "    return D\n",
    "\n",
    "# ======== Segédfüggvények a normalizáláshoz ========\n",
    "def normalize_screenshots_column(df: pd.DataFrame, source_name: str):\n",
    "    \"\"\"\n",
    "    Kivonatolja a screenshots oszlopot (ha létezik) és visszaadja a thumbnail URL-eket.\n",
    "    Működik dict/list/str típusokra is.\n",
    "    \"\"\"\n",
    "    thumb_dict = {}\n",
    "\n",
    "    if \"screenshots\" not in df.columns:\n",
    "        return thumb_dict\n",
    "\n",
    "    for appid, val in df[[\"appid\", \"screenshots\"]].itertuples(index=False):\n",
    "        thumb_urls = []\n",
    "\n",
    "        if val is None:\n",
    "            continue\n",
    "        if isinstance(val, float) and np.isnan(val):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = ast.literal_eval(val) if isinstance(val, str) else val\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict):\n",
    "                    thumb = item.get(\"path_thumbnail\") or item.get(\"thumb\")\n",
    "                    if thumb:\n",
    "                        thumb_urls.append(thumb.strip())\n",
    "                elif isinstance(item, str):\n",
    "                    pass\n",
    "\n",
    "        thumb_urls = [u for u in thumb_urls if isinstance(u, str) and u.startswith(\"http\")]\n",
    "        thumb_urls = list(dict.fromkeys(thumb_urls))\n",
    "\n",
    "        thumb_dict[str(appid)] = thumb_urls\n",
    "\n",
    "    logging.info(f\"Normalized thumbnail screenshots for source {source_name} ({len(thumb_dict)} items)\")\n",
    "    return thumb_dict\n",
    "\n",
    "def process_screenshots(a, b, c):\n",
    "    \"\"\"\n",
    "    Normalizálja a screenshots oszlopokat, \n",
    "    visszaadja a thumbnail dict-eket.\n",
    "    \"\"\"\n",
    "    a_thumb = normalize_screenshots_column(a, \"A\")\n",
    "    b_thumb = normalize_screenshots_column(b, \"B\")\n",
    "    c_thumb = normalize_screenshots_column(c, \"C\")\n",
    "    return a_thumb, b_thumb, c_thumb\n",
    "\n",
    "\n",
    "def normalize_movies_column(df: pd.DataFrame, source_name: str):\n",
    "    '''\n",
    "    Normalizálja a 'movies' oszlopot:\n",
    "    - movies_thumbnail: a 'thumbnail' URL-ek\n",
    "    - movies_480: a 'webm.480' URL-ek\n",
    "    - movies_max: a 'webm.max' URL-ek\n",
    "    '''\n",
    "    thumb_dict = {}\n",
    "    m480_dict = {}\n",
    "    mmax_dict = {}\n",
    "\n",
    "    if \"movies\" not in df.columns:\n",
    "        return thumb_dict, m480_dict, mmax_dict\n",
    "\n",
    "    for appid, val in df[[\"appid\", \"movies\"]].itertuples(index=False):\n",
    "        thumbs = []\n",
    "        webm_480 = []\n",
    "        webm_max = []\n",
    "\n",
    "        if val is None:\n",
    "            continue\n",
    "        if isinstance(val, float) and np.isnan(val):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            data = ast.literal_eval(val) if isinstance(val, str) else val\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if isinstance(data, list):\n",
    "            for item in data:\n",
    "                if isinstance(item, dict):\n",
    "                    t = item.get(\"thumbnail\")\n",
    "                    if t:\n",
    "                        thumbs.append(t.strip())\n",
    "                    w480 = item.get(\"webm\", {}).get(\"480\")\n",
    "                    if w480:\n",
    "                        webm_480.append(w480.strip())\n",
    "                    wmax = item.get(\"webm\", {}).get(\"max\")\n",
    "                    if wmax:\n",
    "                        webm_max.append(wmax.strip())\n",
    "\n",
    "        thumb_dict[str(appid)] = thumbs\n",
    "        m480_dict[str(appid)] = webm_480\n",
    "        mmax_dict[str(appid)] = mmax_dict.get(str(appid), []) + mmax_dict.get(str(appid), [])\n",
    "\n",
    "    logging.info(f\"Normalized movies for source {source_name} ({len(thumb_dict)} items)\")\n",
    "    return thumb_dict, m480_dict, mmax_dict\n",
    "\n",
    "def dedup_join(urls):\n",
    "    '''\n",
    "    Egy lista vagy tuple URL-t megtisztít duplikátumoktól és vesszővel összefűzi őket.\n",
    "    '''\n",
    "    if not urls or not isinstance(urls, (list, tuple)):\n",
    "        return \"\"\n",
    "    return \", \".join(list(dict.fromkeys(urls)))\n",
    "\n",
    "def merge_and_finalize(a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame, columns_to_merge: list[str] = None) -> pd.DataFrame:\n",
    "    '''\n",
    "    Három forrás-DataFrame (A, B, C) egyesítése és véglegesítése.\n",
    "\n",
    "    - Merge-eli a forrásokat az `appid` alapján.\n",
    "    - Kategória-, screenshot- és videóadatokat egyesít és átnevez.\n",
    "    - Thumbnail és 480p videóoszlopokat hoz létre.\n",
    "    - Eltávolítja a duplikált URL-eket (`dedup_join` segítségével).\n",
    "    - Összevonja a fejlesztői, kiadói, kategória- és tag-információkat.\n",
    "    '''\n",
    "    D = merge_sources(a, b, c, columns_to_merge=columns_to_merge)\n",
    "\n",
    "    if 'categories' in a.columns:\n",
    "        D['categories_a'] = D['appid'].map(a.set_index('appid')['categories'])\n",
    "    if 'categories' in b.columns:\n",
    "        D['categories_b'] = D['appid'].map(b.set_index('appid')['categories'])\n",
    "    if 'categories' in c.columns:\n",
    "        D['categories_c'] = D['appid'].map(c.set_index('appid')['categories'])\n",
    "\n",
    "    if \"screenshots\" in D.columns:\n",
    "        D.rename(columns={\"screenshots\": \"screenshots_full\"}, inplace=True)\n",
    "    a_thumb, b_thumb, c_thumb = process_screenshots(a, b, c)\n",
    "    D[\"screenshots_thumb\"] = D[\"appid\"].map(\n",
    "        lambda x: c_thumb.get(x, []) + b_thumb.get(x, []) + a_thumb.get(x, [])\n",
    "    )\n",
    "\n",
    "    if \"movies\" in D.columns:\n",
    "        D.rename(columns={\"movies\": \"movies_max\"}, inplace=True)\n",
    "\n",
    "    a_thumb_m, a_480, a_max = normalize_movies_column(a, \"A\")\n",
    "    b_thumb_m, b_480, b_max = normalize_movies_column(b, \"B\")\n",
    "    c_thumb_m, c_480, c_max = normalize_movies_column(c, \"C\")\n",
    "\n",
    "    D[\"movies_thumbnail\"] = D[\"appid\"].map(lambda x: c_thumb_m.get(x, []) + b_thumb_m.get(x, []) + a_thumb_m.get(x, []))\n",
    "    D[\"movies_480\"] = D[\"appid\"].map(lambda x: c_480.get(x, []) + b_480.get(x, []) + a_480.get(x, []))\n",
    "        \n",
    "\n",
    "    for col in [\"screenshots_thumb\", \"movies_thumbnail\", \"movies_480\"]:\n",
    "        D[col] = D[col].apply(dedup_join)\n",
    "\n",
    "    D = finalize_sources(D, a, b, c)\n",
    "\n",
    "    D = merge_developers_publishers(D)\n",
    "    D = merge_categories(D)\n",
    "\n",
    "    tags_df = merge_tags_column(D, a, b, c)\n",
    "\n",
    "    tags_collapsed = (\n",
    "        tags_df.groupby(\"appid\")\n",
    "        .apply(lambda x: [{\"tag_name\": t, \"weight\": w} for t, w in zip(x[\"tag_name\"], x[\"weight\"])])\n",
    "        .reset_index(name=\"tags\")\n",
    "    )\n",
    "        \n",
    "    D = D.merge(tags_collapsed, on=\"appid\", how=\"left\")\n",
    "\n",
    "    for col in [\"detailed_description\", \"about_the_game\", \"short_description\"]:\n",
    "        if col in D.columns:\n",
    "            D[col] = D[col].astype(str).apply(clean_html_entities)\n",
    "\n",
    "    if \"owners\" in D.columns and \"estimated_owners\" in D.columns:\n",
    "        D[\"estimated_owners\"] = D[\"estimated_owners\"].combine_first(D[\"owners\"])\n",
    "        D.drop(columns=[\"owners\"], inplace=True)\n",
    "    elif \"owners\" in D.columns:\n",
    "        D.rename(columns={\"owners\": \"estimated_owners\"}, inplace=True)\n",
    "\n",
    "    if \"positive\" in D.columns and \"positive_ratings\" in D.columns:\n",
    "        D[\"positive\"] = D[\"positive\"].combine_first(D[\"positive_ratings\"])\n",
    "        D.drop(columns=[\"positive_ratings\"], inplace=True)\n",
    "    elif \"positive_ratings\" in D.columns:\n",
    "        D.rename(columns={\"positive_ratings\": \"positive\"}, inplace=True)\n",
    "    \n",
    "    if \"negative\" in D.columns and \"negative_ratings\" in D.columns:\n",
    "        D[\"negative\"] = D[\"negative\"].combine_first(D[\"negative_ratings\"])\n",
    "        D.drop(columns=[\"negative_ratings\"], inplace=True)\n",
    "    elif \"negative_ratings\" in D.columns:\n",
    "        D.rename(columns={\"negative_ratings\": \"negative\"}, inplace=True)\n",
    "\n",
    "    if \"average_playtime\" in D.columns and \"average_playtime_forever\" in D.columns:\n",
    "        D[\"average_playtime_forever\"] = D[\"average_playtime_forever\"].combine_first(D[\"average_playtime\"])\n",
    "        D.drop(columns=[\"average_playtime\"], inplace=True)\n",
    "    elif \"average_playtime\" in D.columns:\n",
    "        D.rename(columns={\"average_playtime\": \"average_playtime_forever\"}, inplace=True)\n",
    "\n",
    "    if \"median_playtime\" in D.columns and \"median_playtime_forever\" in D.columns:\n",
    "        D[\"median_playtime_forever\"] = D[\"median_playtime_forever\"].combine_first(D[\"median_playtime\"])\n",
    "        D.drop(columns=[\"median_playtime\"], inplace=True)\n",
    "    elif \"median_playtime\" in D.columns:\n",
    "        D.rename(columns={\"median_playtime\": \"median_playtime_forever\"}, inplace=True)\n",
    "\n",
    "    tag_cols = [c for c in D.columns if c.startswith(\"tags\")]\n",
    "    \n",
    "    if len(tag_cols) > 1:\n",
    "        D[\"tags\"] = D[tag_cols].bfill(axis=1).iloc[:, 0]\n",
    "        for c in tag_cols:\n",
    "            if c != \"tags\":\n",
    "                D.drop(columns=c, inplace=True, errors=\"ignore\")\n",
    "        logging.info(f\"Combined duplicate tag columns: {tag_cols} → kept unified 'tags'\")\n",
    "    \n",
    "    if \"tags\" in D.columns:\n",
    "        D[\"tags\"] = D[\"tags\"].apply(normalize_tags_column)\n",
    "\n",
    "    for col in D.columns:\n",
    "        if \"steamspy\" in col.lower() and \"tag\" in col.lower():\n",
    "            D.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
    "            logging.info(f\"Dropped redundant column: {col}\")\n",
    "\n",
    "    for col in [\"supported_languages\", \"full_audio_languages\"]:\n",
    "        if col in D.columns:\n",
    "            D[col] = D[col].apply(clean_language_field_merged)\n",
    "            logging.info(f\"Cleaned and normalized language field: {col}\")\n",
    "\n",
    "            \n",
    "    for col in [\"supported_languages\", \"full_audio_languages\"]:\n",
    "        if col in D.columns:\n",
    "            before_counts = D[col].apply(len).sum()\n",
    "            D[col] = D[col].apply(final_clean_language_list)\n",
    "            after_counts = D[col].apply(len).sum()\n",
    "            logging.info(\n",
    "                f\"Final language cleanup on {col}: {before_counts} → {after_counts} entries (after filtering).\"\n",
    "            )\n",
    "\n",
    "    if \"supported_languages\" in D.columns and \"full_audio_languages\" in D.columns:\n",
    "        identical_rows = (D[\"supported_languages\"].astype(str) == D[\"full_audio_languages\"].astype(str)).sum()\n",
    "        logging.info(f\"{identical_rows} rows have identical supported and audio language sets.\")\n",
    "\n",
    "    return D\n",
    "    \n",
    "def flatten_values(vals):\n",
    "    \"\"\"Lapítja a listákat / stringként tárolt listákat egy sima listává.\"\"\"\n",
    "    flat = []\n",
    "    for v in vals:\n",
    "        if isinstance(v, str):\n",
    "            v = v.strip()\n",
    "            if v.startswith(\"[\") and v.endswith(\"]\"):\n",
    "                try:\n",
    "                    sublist = ast.literal_eval(v)\n",
    "                    if isinstance(sublist, list):\n",
    "                        flat.extend([str(s).strip() for s in sublist if pd.notna(s)])\n",
    "                        continue\n",
    "                except Exception:\n",
    "                    pass\n",
    "        flat.append(str(v).strip())\n",
    "    return list(dict.fromkeys(flat))\n",
    "\n",
    "def combine_cols(row: pd.Series, cols: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Több oszlopból származó értékeket kombinál egyetlen, duplikátummentes stringgé.\n",
    "\n",
    "    - Kinyeri az értékeket a megadott oszlopokból.\n",
    "    - Támogatja a listákat, NumPy tömböket és skalárokat is.\n",
    "    - A duplikátumokat eltávolítja és vesszővel elválasztva adja vissza.\n",
    "    \"\"\"\n",
    "    vals = []\n",
    "    for col in cols:\n",
    "        val = row.get(col, None)\n",
    "        if val is None:\n",
    "            continue\n",
    "        if isinstance(val, (list, np.ndarray)):\n",
    "            vals.extend(flatten_values(val))\n",
    "        else:\n",
    "            vals.extend(flatten_values([val]))\n",
    "    return \", \".join(list(dict.fromkeys(vals)))\n",
    "\n",
    "\n",
    "def merge_developers_publishers(D: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Összevonja a fejlesztői és kiadói oszlopokat, eltávolítva a duplikált neveket.\n",
    "\n",
    "    - A 'developer' és 'developers' oszlopokból egyesített 'developers' oszlopot hoz létre.\n",
    "    - A 'publisher' és 'publishers' oszlopokból egyesített 'publishers' oszlopot hoz létre.\n",
    "    - Az eredeti ('developer', 'publisher') oszlopokat eltávolítja.\n",
    "    \"\"\"\n",
    "    D[\"developers\"] = D.apply(lambda row: combine_cols(row, [\"developer\", \"developers\"]), axis=1)\n",
    "    D[\"publishers\"] = D.apply(lambda row: combine_cols(row, [\"publisher\", \"publishers\"]), axis=1)\n",
    "\n",
    "    for col in [\"developer\", \"publisher\"]:\n",
    "        if col in D.columns:\n",
    "            D.drop(columns=[col], inplace=True)\n",
    "\n",
    "    return D\n",
    "\n",
    "def parse_categories(val) -> list[str]:\n",
    "    \"\"\"\n",
    "    Kategóriaértékek egységes listává alakítása.\n",
    "\n",
    "    - Kezeli a listákat, NumPy tömböket, stringeket és None értékeket.\n",
    "    - Tisztítja az üres vagy NaN értékeket.\n",
    "    - Felismeri a stringként tárolt listákat és a pontosvesszővel tagolt formátumokat.\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, (float, np.floating)) and np.isnan(val):\n",
    "        return []\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        return [str(v).strip() for v in val if isinstance(v, str) and v.strip()]\n",
    "    if isinstance(val, str):\n",
    "        val = val.strip()\n",
    "        if not val:\n",
    "            return []\n",
    "        if val.startswith(\"[\") and val.endswith(\"]\"):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(val)\n",
    "                if isinstance(parsed, list):\n",
    "                    return [str(v).strip() for v in parsed if isinstance(v, str) and v.strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        if \";\" in val:\n",
    "            return [v.strip() for v in val.split(\";\") if v.strip()]\n",
    "        return [val]\n",
    "    return []\n",
    "\n",
    "def combine_categories(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Egy sor kategóriaoszlopait (A, B, C) kombinálja egyetlen, duplikátummentes stringgé.\n",
    "    \"\"\"\n",
    "    cats_a = parse_categories(row.get(\"categories_a\", row.get(\"categories\", None)))\n",
    "    cats_b = parse_categories(row.get(\"categories_b\", None))\n",
    "    cats_c = parse_categories(row.get(\"categories_c\", None))\n",
    "\n",
    "    merged = []\n",
    "    seen_lower = set()\n",
    "\n",
    "    for c in cats_a + cats_b + cats_c:\n",
    "        cl = c.lower()\n",
    "        if cl not in seen_lower:\n",
    "            merged.append(c)\n",
    "            seen_lower.add(cl)\n",
    "\n",
    "    return \", \".join(merged)\n",
    "\n",
    "def merge_categories(D: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A források kategóriaoszlopait egyesíti egységes 'categories' oszlopba.\n",
    "\n",
    "    - A 'categories_a', 'categories_b', 'categories_c' oszlopokat kombinálja.\n",
    "    - Duplikátumokat kiszűri, kisbetű-érzéketlen módon.\n",
    "    - Eltávolítja a felesleges kategóriaoszlopokat.\n",
    "    \"\"\"\n",
    "    category_cols = [c for c in D.columns if \"categor\" in c.lower()]\n",
    "    D[\"categories\"] = D.apply(combine_categories, axis=1)\n",
    "\n",
    "    for col in category_cols:\n",
    "        if col != \"categories\":\n",
    "            D.drop(columns=[col], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "def merge_tags_column(D: pd.DataFrame, a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Egyesíti a címkék (tags) adatokat az A, B, C forrásokból AppID alapján.\n",
    "\n",
    "    - Az A forrásban a tags stringként tárolt, vesszővel elválasztott lista.\n",
    "    - A B forrásban a tags dict formátumú (név → súly).\n",
    "    - A C forrásban vegyes formátumot kezel (stringként tárolt dict is lehet).\n",
    "    - Az eredmény egy DataFrame, ami appid, tag_name, weight oszlopokat tartalmaz.\n",
    "    \"\"\"\n",
    "    tags_a_dict = {}\n",
    "    if 'tags' in a.columns:\n",
    "        for appid, tags_str in zip(a['appid'], a['tags']):\n",
    "            if isinstance(tags_str, str):\n",
    "                tags_list = [t.strip() for t in tags_str.split(\",\") if t.strip()]\n",
    "                tags_a_dict[str(appid)] = {t: 1 for t in tags_list} \n",
    "\n",
    "    tags_b_dict = {}\n",
    "    if 'tags' in b.columns:\n",
    "        for appid, tags_json in zip(b['appid'], b['tags']):\n",
    "            if isinstance(tags_json, dict):\n",
    "                tags_b_dict[str(appid)] = tags_json\n",
    "\n",
    "    tags_c_dict = {}\n",
    "    if 'tags' in c.columns:\n",
    "        for appid, tags_str in zip(c['appid'], c['tags']):\n",
    "            if isinstance(tags_str, str):\n",
    "                try:\n",
    "                    tags_dict = ast.literal_eval(tags_str)\n",
    "                    if isinstance(tags_dict, dict):\n",
    "                        tags_c_dict[str(appid)] = tags_dict\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    tag_rows = []\n",
    "    for appid in D['appid']:\n",
    "        tag_dict = {}\n",
    "        tag_dict.update(tags_a_dict.get(str(appid), {}))\n",
    "        tag_dict.update(tags_b_dict.get(str(appid), {}))\n",
    "        tag_dict.update(tags_c_dict.get(str(appid), {}))\n",
    "\n",
    "        for t, w in tag_dict.items():\n",
    "            tag_rows.append({\"appid\": appid, \"tag_name\": t, \"weight\": w})\n",
    "\n",
    "    tags_df = pd.DataFrame(tag_rows)\n",
    "    return tags_df\n",
    "# =========================================\n",
    "# VISUALIZATION FUNCTION\n",
    "# =========================================\n",
    "def compute_venn_sets(a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame):\n",
    "    \"\"\"Kiszámítja a halmazokat az appid alapján.\"\"\"\n",
    "    set_a = set(a[\"appid\"].astype(str))\n",
    "    set_b = set(b[\"appid\"].astype(str))\n",
    "    set_c = set(c[\"appid\"].astype(str))\n",
    "\n",
    "    only_a = set_a - set_b - set_c\n",
    "    only_b = set_b - set_a - set_c\n",
    "    only_c = set_c - set_a - set_b\n",
    "\n",
    "    a_and_b = (set_a & set_b) - set_c\n",
    "    a_and_c = (set_a & set_c) - set_b\n",
    "    b_and_c = (set_b & set_c) - set_a\n",
    "\n",
    "    all_three = set_a & set_b & set_c\n",
    "\n",
    "    return {\n",
    "        \"only_a\": only_a,\n",
    "        \"only_b\": only_b,\n",
    "        \"only_c\": only_c,\n",
    "        \"a_and_b\": a_and_b,\n",
    "        \"a_and_c\": a_and_c,\n",
    "        \"b_and_c\": b_and_c,\n",
    "        \"all_three\": all_three\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_venn_table(a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Visszaadja az elemszámokat egy táblázatban.\"\"\"\n",
    "    s = compute_venn_sets(a, b, c)\n",
    "    data = {\n",
    "        \"csak A\": [len(s[\"only_a\"])],\n",
    "        \"csak B\": [len(s[\"only_b\"])],\n",
    "        \"csak C\": [len(s[\"only_c\"])],\n",
    "        \"A ∩ B\": [len(s[\"a_and_b\"])],\n",
    "        \"A ∩ C\": [len(s[\"a_and_c\"])],\n",
    "        \"B ∩ C\": [len(s[\"b_and_c\"])],\n",
    "        \"A ∩ B ∩ C\": [len(s[\"all_three\"])]\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def plot_and_save_venn(a: pd.DataFrame, b: pd.DataFrame, c: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"Létrehozza és elmenti a Venn-diagramot és a táblázatot.\"\"\"\n",
    "    s = compute_venn_sets(a, b, c)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    venn3(\n",
    "        subsets=(\n",
    "            len(s[\"only_a\"]), len(s[\"only_b\"]), len(s[\"a_and_b\"]),\n",
    "            len(s[\"only_c\"]), len(s[\"a_and_c\"]), len(s[\"b_and_c\"]),\n",
    "            len(s[\"all_three\"])\n",
    "        ),\n",
    "        set_labels=(\"Forrás A\", \"Forrás B\", \"Forrás C\"),\n",
    "        set_colors=(\"skyblue\", \"lightgreen\", \"lightcoral\"),\n",
    "        alpha=0.8\n",
    "    )\n",
    "    plt.title(\"Adatforrások átfedése – Venn-diagram\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    venn_path = os.path.join(output_dir, \"venn_diagram.png\")\n",
    "    plt.savefig(venn_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Venn-diagram mentve ide: {venn_path}\")\n",
    "\n",
    "    venn_df = compute_venn_table(a, b, c)\n",
    "    table_path = os.path.join(output_dir, \"venn_table.csv\")\n",
    "    venn_df.to_csv(table_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Elemszámos Venn-táblázat mentve ide: {table_path}\")\n",
    "\n",
    "    print(\"\\n=== Elemszámos Venn-diagram táblázat ===\")\n",
    "    print(venn_df.to_string(index=False))\n",
    "\n",
    "# =========================================\n",
    "# SOURCE SUMMARY TABLE\n",
    "# =========================================\n",
    "def save_source_summary(D: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"\n",
    "    Összesítő táblázatot készít arról, hogy hány rekord származik\n",
    "    csak A-ból, csak B-ből, csak C-ből, illetve ezek kombinációiból.\n",
    "    Az eredményt CSV-be menti és ki is írja a konzolra.\n",
    "    \"\"\"\n",
    "    if \"sources\" not in D.columns:\n",
    "        logging.warning(\"A 'sources' oszlop nem található a merged táblában.\")\n",
    "        return\n",
    "\n",
    "    summary = D[\"sources\"].value_counts().reset_index()\n",
    "    summary.columns = [\"forrás_kombináció\", \"rekordok_száma\"]\n",
    "\n",
    "    summary[\"tartalmaz_A\"] = summary[\"forrás_kombináció\"].str.contains(\"A\", regex=False)\n",
    "    summary[\"tartalmaz_B\"] = summary[\"forrás_kombináció\"].str.contains(\"B\", regex=False)\n",
    "    summary[\"tartalmaz_C\"] = summary[\"forrás_kombináció\"].str.contains(\"C\", regex=False)\n",
    "\n",
    "    output_file = os.path.join(output_dir, \"source_summary.csv\")\n",
    "    summary.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    logging.info(f\"Source summary saved: {output_file}\")\n",
    "    print(\"\\n=== Forrásonkénti rekordösszesítő táblázat ===\")\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "def validate_integrity(D: pd.DataFrame, output_dir: str):\n",
    "    \"\"\"\n",
    "    Adatintegritás-ellenőrzés: duplikált appid, hiányzó értékek, típushibák stb.\n",
    "    Eredményt logolja és CSV-be menti.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    dup_count = D[\"appid\"].duplicated().sum()\n",
    "    results.append({\"ellenőrzés\": \"Duplikált appid-ek\", \"hibák_száma\": dup_count})\n",
    "\n",
    "    na_appid = D[\"appid\"].isna().sum()\n",
    "    results.append({\"ellenőrzés\": \"Hiányzó appid-ek\", \"hibák_száma\": na_appid})\n",
    "\n",
    "    if \"name\" in D.columns:\n",
    "        na_name = D[\"name\"].isna().sum()\n",
    "        results.append({\"ellenőrzés\": \"Hiányzó játéknevek\", \"hibák_száma\": na_name})\n",
    "\n",
    "    if \"sources\" in D.columns:\n",
    "        na_sources = (D[\"sources\"] == \"\").sum()\n",
    "        results.append({\"ellenőrzés\": \"Hiányzó forrásjelölés\", \"hibák_száma\": na_sources})\n",
    "\n",
    "    if \"release_date\" in D.columns:\n",
    "        invalid_dates = pd.to_datetime(D[\"release_date\"], errors=\"coerce\").isna().sum()\n",
    "        results.append({\"ellenőrzés\": \"Érvénytelen release_date\", \"hibák_száma\": invalid_dates})\n",
    "\n",
    "    integrity_df = pd.DataFrame(results)\n",
    "    output_file = os.path.join(output_dir, \"integrity_report.csv\")\n",
    "    integrity_df.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    logging.info(f\"Integrity check completed, saved to {output_file}\")\n",
    "    print(\"\\n=== Integritás ellenőrzési összesítő ===\")\n",
    "    print(integrity_df.to_string(index=False))\n",
    "\n",
    "\n",
    "def save_merged(D, path):\n",
    "    \"\"\"\n",
    "    Elmenti az egyesített (merged_master) táblát a megadott könyvtárba UTF-8 kódolással.\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(path, \"merged_master.csv\")\n",
    "    D.to_csv(output_file, index=False, encoding=\"utf-8-sig\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# ======== MAIN ========\n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    logging.info(\"=== Starting merge process ===\")\n",
    "    a = load_source_a(A_PATH)\n",
    "    a_output_file = os.path.join(OUTPUT_PATH, \"A_merged.csv\")\n",
    "    a.to_csv(a_output_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    b = load_source_b(B_PATH)\n",
    "\n",
    "    b.to_csv(os.path.join(OUTPUT_PATH, \"B_full.csv\"), index=False, encoding=\"utf-8\")\n",
    "    logging.info(\"B full DataFrame written to CSV\")\n",
    "    \n",
    "    c = load_source_c(C_PATH)\n",
    "\n",
    "    plot_and_save_venn(a, b, c, OUTPUT_PATH)\n",
    "\n",
    "    D = merge_and_finalize(a, b, c)\n",
    "\n",
    "    #D = merge_and_finalize(a, b, c, columns_to_merge = [\"owners\"])\n",
    "\n",
    "    output_file = save_merged(D, OUTPUT_PATH)\n",
    "    save_source_summary(D, OUTPUT_PATH)\n",
    "    validate_integrity(D, OUTPUT_PATH)\n",
    "    logging.info(f\"Merged master table saved to: {output_file}\")\n",
    "    \n",
    "    logging.info(\"=== Merge process successfully completed ===\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-09 11:46:03] INFO: === Starting splitting process ===\n",
      "C:\\Users\\zalma\\AppData\\Local\\Temp\\ipykernel_11852\\1140561955.py:37: DtypeWarning: Columns (4,15,20,21,22,23,24,28,29,30,54,55,56) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, **kwargs)\n",
      "[2025-11-09 11:46:11] INFO: Loaded: merged_master.csv (112855 rows)\n",
      "[2025-11-09 11:46:11] INFO: Saved 'media.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:46:15] INFO: Saved 'screenshots.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:46:16] INFO: Saved 'movies.csv' (112771 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:46:16] INFO: Saved 'support.csv' (105409 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:05] INFO: Saved 'requirements.csv' (72365 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:11] INFO: Saved 'platforms.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:11] INFO: Saved 'game_platform.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:23] INFO: Saved game_package.csv (88410 rows)\n",
      "[2025-11-09 11:47:23] INFO: Saved packages.csv (88410 rows)\n",
      "[2025-11-09 11:47:23] INFO: Saved sub_package.csv (93939 rows)\n",
      "[2025-11-09 11:47:29] INFO: Saved 'developers.csv' (106337 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:29] INFO: Saved 'game_developer.csv' (106337 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:35] INFO: Saved 'publishers.csv' (106868 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:35] INFO: Saved 'game_publisher.csv' (106868 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:41] INFO: Saved 'genres.csv' (106341 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:41] INFO: Saved 'game_genre.csv' (106341 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:46] INFO: Saved 'categories.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:47:46] INFO: Saved 'game_category.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:48:13] INFO: Saved 'game_tag.csv' (1116858 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:48:13] INFO: Saved 'tags.csv' (1116858 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:48:19] INFO: Saved 'description.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:48:21] INFO: Saved 'game.csv' (112855 rows) to C:\\Users\\zalma\\split\n",
      "[2025-11-09 11:48:34] INFO: Saved languages (109), game_subtitles (542160), game_audio_language (212736) to C:\\Users\\zalma\\split\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Any\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# ======== PATHS ========\n",
    "BASE_PATH = r\"C:\\Users\\zalma\"\n",
    "D_PATH = os.path.join(BASE_PATH, \"merge\")\n",
    "OUTPUT_PATH = os.path.join(BASE_PATH, \"split\")\n",
    "\n",
    "# ======== LOGGING CONFIGURATION ========\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "LOG_FILE = os.path.join(OUTPUT_PATH, \"merge_log.txt\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.FileHandler(LOG_FILE, encoding=\"utf-8\"), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "# ======== HELPER FUNCTIONS ========\n",
    "def load_csv_safely(path: str, **kwargs: Any) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Betölt egy CSV fájlt, hiba esetén üres DataFrame-et ad vissza.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        logging.info(f\"Loaded: {os.path.basename(path)} ({len(df)} rows)\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# ======== Segédfüggvények a splittelt táblákhoz ========\n",
    "def create_media_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a media táblát a merged_master-ből.\n",
    "    \n",
    "    \"\"\"\n",
    "    media_cols = [\"appid\", \"header_image\", \"background\"]\n",
    "    media_df = master_df[[c for c in media_cols if c in master_df.columns]].copy()\n",
    "    \n",
    "    media_df = media_df.dropna(subset=[\"header_image\"]).reset_index(drop=True)\n",
    "    \n",
    "    media_df.insert(0, \"mediaid\", range(1, len(media_df)+1))\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"media.csv\")\n",
    "        media_df.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'media.csv' ({len(media_df)} rows) to {output_dir}\")\n",
    "    \n",
    "    return media_df\n",
    "\n",
    "def join_urls(x) -> str:\n",
    "    \"\"\"\n",
    "    Lista vagy string URL-eket egységes, vesszővel elválasztott stringgé alakít.\n",
    "\n",
    "    - Ha lista, akkor elemeit összefűzi ', ' elválasztóval.\n",
    "    - Ha már string, változatlanul visszaadja.\n",
    "    - Egyéb esetben üres stringet ad vissza.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return \", \".join(x)\n",
    "    elif isinstance(x, str):\n",
    "        return x\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def create_screenshots_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a screenshots táblát a master DataFrame-ből.\n",
    "\n",
    "    - Kiválasztja az 'appid', 'screenshots_full' és 'screenshots_thumb' oszlopokat.\n",
    "    - A listákat stringgé alakítja (`join_urls` segítségével).\n",
    "    - Eltávolítja az üres sorokat.\n",
    "    - Hozzáad egy automatikus 'screenshotid' azonosítót.\n",
    "    - CSV-fájlba menti az eredményt.\n",
    "    \"\"\"\n",
    "    cols = [\"appid\"]\n",
    "    for c in [\"screenshots_full\", \"screenshots_thumb\"]:\n",
    "        if c in master_df.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    df = master_df[cols].copy()\n",
    "\n",
    "    for c in [\"screenshots_full\", \"screenshots_thumb\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(join_urls)\n",
    "\n",
    "    df = df[\n",
    "        (df.get(\"screenshots_full\", \"\") != \"\") |\n",
    "        (df.get(\"screenshots_thumb\", \"\") != \"\")\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    df.insert(0, \"screenshotid\", range(1, len(df) + 1))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"screenshots.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'screenshots.csv' ({len(df)} rows) to {output_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_movies_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a 'movies' táblát a master DataFrame-ből.\n",
    "\n",
    "    - Kiválasztja az 'appid', 'movies_thumbnail', 'movies_max' és 'movies_480' oszlopokat.\n",
    "    - A listákat stringgé alakítja (`join_urls` segítségével).\n",
    "    - Csak azokat a sorokat tartja meg, ahol legalább egy URL szerepel.\n",
    "    - Hozzáad egy automatikus 'movieid' azonosítót.\n",
    "    - (Opcionálisan) CSV-fájlba menti az eredményt.\n",
    "\n",
    "    Visszatér: a videókat tartalmazó DataFrame.\n",
    "    \"\"\"\n",
    "    cols = [\"appid\"]\n",
    "    for c in [\"movies_thumbnail\", \"movies_max\", \"movies_480\"]:\n",
    "        if c in master_df.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    df = master_df[cols].copy()\n",
    "\n",
    "    for c in [\"movies_thumbnail\", \"movies_max\", \"movies_480\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(join_urls)\n",
    "\n",
    "    df = df[\n",
    "        (df.get(\"movies_thumbnail\", \"\") != \"\") |\n",
    "        (df.get(\"movies_max\", \"\") != \"\") |\n",
    "        (df.get(\"movies_480\", \"\") != \"\")\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    df.insert(0, \"movieid\", range(1, len(df) + 1))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"movies.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'movies.csv' ({len(df)} rows) to {output_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_support_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a support táblát a merged_master-ből.\n",
    "    Tartalmazza:\n",
    "      - supportid (1-től generált)\n",
    "      - appid\n",
    "      - support_url\n",
    "      - support_email\n",
    "    \"\"\"\n",
    "    cols = [\"appid\"]\n",
    "    for c in [\"support_url\", \"support_email\"]:\n",
    "        if c in master_df.columns:\n",
    "            cols.append(c)\n",
    "\n",
    "    df = master_df[cols].copy()\n",
    "\n",
    "    for c in [\"support_url\", \"support_email\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(\"\").astype(str)\n",
    "\n",
    "    df = df[(df.get(\"support_url\", \"\") != \"\") | (df.get(\"support_email\", \"\") != \"\")].reset_index(drop=True)\n",
    "\n",
    "    df.insert(0, \"supportid\", range(1, len(df) + 1))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"support.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'support.csv' ({len(df)} rows) to {output_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_requirements_text(text):\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(str(text), \"html.parser\")\n",
    "\n",
    "    for br in soup.find_all(\"br\"):\n",
    "        br.replace_with(\" \")\n",
    "\n",
    "    for li in soup.find_all(\"li\"):\n",
    "        li.replace_with(f\"{li.get_text()}, \")\n",
    "\n",
    "    cleaned = soup.get_text(separator=\" \").strip()\n",
    "\n",
    "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "    cleaned = re.sub(r',\\s*$', '', cleaned)\n",
    "\n",
    "    cleaned = re.sub(r'^[\\)\\(\"\\'\\s,]+', '', cleaned)\n",
    "\n",
    "    cleaned = re.sub(r'(?i)^(minimum|recommended)[:\\s-]*', '', cleaned).strip()\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def split_min_rec(text):\n",
    "    \"\"\"\n",
    "    Szétválasztja a minimum és recommended részt a stringből.\n",
    "    Kis-/nagybetűt normalizál, ha a minimumban benne van a recommended, szétvágja.\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\", \"\"\n",
    "    text = str(text).strip()\n",
    "    parts = re.split(r\"(?i)Recommended[:\\s]*\", text, maxsplit=1)\n",
    "    min_part = parts[0].strip() if parts else \"\"\n",
    "    rec_part = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    return min_part, rec_part\n",
    "\n",
    "\n",
    "def create_requirements_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    for _, r in master_df.iterrows():\n",
    "        appid = r['appid']\n",
    "\n",
    "        # --- Windows (pc_requirements) ---\n",
    "        pc_val = r.get('pc_requirements', \"\")\n",
    "        if pd.notna(pc_val) and str(pc_val).strip():\n",
    "            try:\n",
    "                val_dict = ast.literal_eval(pc_val)\n",
    "                win_min = clean_requirements_text(val_dict.get('minimum', \"\"))\n",
    "                win_rec = clean_requirements_text(val_dict.get('recommended', \"\"))\n",
    "            except Exception:\n",
    "                text = clean_requirements_text(pc_val)\n",
    "                win_min, win_rec = split_min_rec(text)\n",
    "\n",
    "            if win_min:\n",
    "                win_min, extra_rec = split_min_rec(win_min)\n",
    "                if win_min:\n",
    "                    rows.append({'appid': appid, 'os': 'windows', 'type': 'minimum', 'requirements': win_min})\n",
    "                if extra_rec:\n",
    "                    rows.append({'appid': appid, 'os': 'windows', 'type': 'recommended', 'requirements': extra_rec})\n",
    "\n",
    "            if win_rec:\n",
    "                rows.append({'appid': appid, 'os': 'windows', 'type': 'recommended', 'requirements': win_rec})\n",
    "\n",
    "\n",
    "        # --- Mac ---\n",
    "        val = r.get('mac_requirements', \"\")\n",
    "        if pd.notna(val):\n",
    "            val_str = str(val).strip()\n",
    "            if val_str and val_str not in [\"[]\", \"{}\", \"nan\", \"None\"]:\n",
    "                try:\n",
    "                    val_dict = ast.literal_eval(val)\n",
    "                    min_val = val_dict.get('minimum', \"\")\n",
    "                    rec_val = val_dict.get('recommended', \"\")\n",
    "                except Exception:\n",
    "                    min_val = val\n",
    "                    rec_val = \"\"\n",
    "\n",
    "                min_val = clean_requirements_text(min_val)\n",
    "                rec_val = clean_requirements_text(rec_val)\n",
    "\n",
    "                if min_val:\n",
    "                    min_val, extra_rec = split_min_rec(min_val)\n",
    "                    if min_val:\n",
    "                        rows.append({'appid': appid, 'os': 'mac', 'type': 'minimum', 'requirements': min_val})\n",
    "                    if extra_rec:\n",
    "                        rows.append({'appid': appid, 'os': 'mac', 'type': 'recommended', 'requirements': extra_rec})\n",
    "                if rec_val:\n",
    "                    rows.append({'appid': appid, 'os': 'mac', 'type': 'recommended', 'requirements': rec_val})\n",
    "\n",
    "        # --- Linux ---\n",
    "        val = r.get('linux_requirements', \"\")\n",
    "        if pd.notna(val):\n",
    "            val_str = str(val).strip()\n",
    "            if val_str and val_str not in [\"[]\", \"{}\", \"nan\", \"None\"]:\n",
    "                try:\n",
    "                    val_dict = ast.literal_eval(val)\n",
    "                    min_val = val_dict.get('minimum', \"\")\n",
    "                    rec_val = val_dict.get('recommended', \"\")\n",
    "                except Exception:\n",
    "                    min_val = val\n",
    "                    rec_val = \"\"\n",
    "\n",
    "                min_val = clean_requirements_text(min_val)\n",
    "                rec_val = clean_requirements_text(rec_val)\n",
    "\n",
    "                if min_val:\n",
    "                    min_val, extra_rec = split_min_rec(min_val)\n",
    "                    if min_val:\n",
    "                        rows.append({'appid': appid, 'os': 'linux', 'type': 'minimum', 'requirements': min_val})\n",
    "                    if extra_rec:\n",
    "                        rows.append({'appid': appid, 'os': 'linux', 'type': 'recommended', 'requirements': extra_rec})\n",
    "                if rec_val:\n",
    "                    rows.append({'appid': appid, 'os': 'linux', 'type': 'recommended', 'requirements': rec_val})\n",
    "\n",
    "\n",
    "\n",
    "    df_req = pd.DataFrame(rows)\n",
    "    if not df_req.empty:\n",
    "        df_req.insert(0, 'reqid', range(1, len(df_req)+1))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"requirements.csv\")\n",
    "        df_req.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'requirements.csv' ({len(df_req)} rows) to {output_dir}\")\n",
    "\n",
    "    return df_req\n",
    "\n",
    "\n",
    "def create_genres_table(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Létrehozza a game_genre és genres táblákat:\n",
    "    - game_genre: appid + genreid\n",
    "    - genres: genreid + genre_name (eredeti genres mező)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        genres_raw = row.get(\"genres\", \"\")\n",
    "\n",
    "        text = str(genres_raw).strip()\n",
    "        if text in [\"\", \"[]\", \"['']\"]:\n",
    "            continue\n",
    "\n",
    "        rows.append({\"appid\": appid, \"genre_name\": text})\n",
    "\n",
    "    df_flat = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    df_flat.insert(1, \"genreid\", range(1, len(df_flat)+1))\n",
    "\n",
    "    game_genre_df = df_flat[['appid', 'genreid']].copy()\n",
    "    genres_df = df_flat[['genreid', 'genre_name']].copy()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        genres_path = os.path.join(output_dir, \"genres.csv\")\n",
    "        game_genre_path = os.path.join(output_dir, \"game_genre.csv\")\n",
    "        genres_df.to_csv(genres_path, index=False)\n",
    "        game_genre_df.to_csv(game_genre_path, index=False)\n",
    "        logging.info(f\"Saved 'genres.csv' ({len(genres_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'game_genre.csv' ({len(game_genre_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return genres_df, game_genre_df\n",
    "\n",
    "def create_platforms_table(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Létrehozza a game_platform és platforms táblákat:\n",
    "    - game_platform: appid + platid\n",
    "    - platforms: platid + windows/linux/mac logikai mezők\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        windows = bool(row.get(\"windows\", False))\n",
    "        linux = bool(row.get(\"linux\", False))\n",
    "        mac = bool(row.get(\"mac\", False))\n",
    "\n",
    "        rows.append({\n",
    "            \"appid\": appid,\n",
    "            \"windows\": windows,\n",
    "            \"linux\": linux,\n",
    "            \"mac\": mac\n",
    "        })\n",
    "\n",
    "    df_flat = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    df_flat.insert(1, \"platid\", range(1, len(df_flat)+1))\n",
    "\n",
    "    game_platform_df = df_flat[['appid', 'platid']].copy()\n",
    "    platforms_df = df_flat[['platid', 'windows', 'linux', 'mac']].copy()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        platforms_path = os.path.join(output_dir, \"platforms.csv\")\n",
    "        game_platform_path = os.path.join(output_dir, \"game_platform.csv\")\n",
    "        platforms_df.to_csv(platforms_path, index=False)\n",
    "        game_platform_df.to_csv(game_platform_path, index=False)\n",
    "        logging.info(f\"Saved 'platforms.csv' ({len(platforms_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'game_platform.csv' ({len(game_platform_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return platforms_df, game_platform_df\n",
    "\n",
    "def create_packages_table(master_df: pd.DataFrame, output_dir: str):\n",
    "    rows_game_package = []\n",
    "    rows_packages = []\n",
    "    rows_sub_package = []\n",
    "\n",
    "    packid_counter = 1\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        packages_raw = row.get(\"packages\", \"\")\n",
    "\n",
    "        if pd.isna(packages_raw) or not str(packages_raw).strip():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            packages_list = ast.literal_eval(packages_raw)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if not isinstance(packages_list, list):\n",
    "            continue\n",
    "\n",
    "        for pkg in packages_list:\n",
    "            title = pkg.get(\"title\", \"\").strip()\n",
    "            description = pkg.get(\"description\", \"\").strip()\n",
    "\n",
    "            if not title:\n",
    "                continue\n",
    "\n",
    "            rows_game_package.append({\"appid\": appid, \"packid\": packid_counter})\n",
    "\n",
    "            rows_packages.append({\"packid\": packid_counter, \"title\": title, \"description\": description})\n",
    "\n",
    "            subs = pkg.get(\"subs\", [])\n",
    "            for sub in subs:\n",
    "                sub_text = sub.get(\"text\", \"\").strip()\n",
    "                price = sub.get(\"price\", None)\n",
    "                rows_sub_package.append({\"packid\": packid_counter, \"sub_text\": sub_text, \"price\": price})\n",
    "\n",
    "            packid_counter += 1\n",
    "\n",
    "    df_game_package = pd.DataFrame(rows_game_package)\n",
    "    df_packages = pd.DataFrame(rows_packages)\n",
    "    df_sub_package = pd.DataFrame(rows_sub_package)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_game_package.to_csv(os.path.join(output_dir, \"game_package.csv\"), index=False)\n",
    "    df_packages.to_csv(os.path.join(output_dir, \"packages.csv\"), index=False)\n",
    "    df_sub_package.to_csv(os.path.join(output_dir, \"sub_package.csv\"), index=False)\n",
    "\n",
    "    logging.info(f\"Saved game_package.csv ({len(df_game_package)} rows)\")\n",
    "    logging.info(f\"Saved packages.csv ({len(df_packages)} rows)\")\n",
    "    logging.info(f\"Saved sub_package.csv ({len(df_sub_package)} rows)\")\n",
    "\n",
    "    return df_game_package, df_packages, df_sub_package\n",
    "\n",
    "def create_developer_tables(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Létrehozza a developers és game_developer táblákat úgy,\n",
    "    hogy minden játékhoz egy sor tartozik, még ha több fejlesztője is van.\n",
    "    - game_developer: appid + devid (1-től generált)\n",
    "    - developers: devid + name (összefűzött fejlesztők)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        devs_raw = row.get(\"developers\", \"\")\n",
    "        if not devs_raw or pd.isna(devs_raw):\n",
    "            continue\n",
    "        \n",
    "        if isinstance(devs_raw, list):\n",
    "            dev_list = [str(d).strip() for d in devs_raw if str(d).strip()]\n",
    "        else:\n",
    "            dev_list = [d.strip() for d in str(devs_raw).split(\",\") if d.strip()]\n",
    "\n",
    "        if not dev_list:\n",
    "            continue\n",
    "        \n",
    "        combined_devs = \", \".join(dev_list)\n",
    "        rows.append({\"appid\": appid, \"developer_name\": combined_devs})\n",
    "\n",
    "    df_flat = pd.DataFrame(rows).reset_index(drop=True)\n",
    "\n",
    "    df_flat.insert(1, \"devid\", range(1, len(df_flat)+1))\n",
    "\n",
    "    game_developer_df = df_flat[['appid', 'devid']].copy()\n",
    "    developers_df = df_flat[['devid', 'developer_name']].copy()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        developers_path = os.path.join(output_dir, \"developers.csv\")\n",
    "        game_developer_path = os.path.join(output_dir, \"game_developer.csv\")\n",
    "        developers_df.to_csv(developers_path, index=False)\n",
    "        game_developer_df.to_csv(game_developer_path, index=False)\n",
    "        logging.info(f\"Saved 'developers.csv' ({len(developers_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'game_developer.csv' ({len(game_developer_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return developers_df, game_developer_df\n",
    "\n",
    "def create_publisher_tables(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Létrehozza a publishers és game_publisher táblákat úgy,\n",
    "    hogy minden játékhoz egy sor tartozik, még ha több kiadója is van.\n",
    "    - game_publisher: appid + pubid (1-től generált)\n",
    "    - publishers: pubid + name (összefűzött kiadók)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        pubs_raw = row.get(\"publishers\", \"\")\n",
    "        if not pubs_raw or pd.isna(pubs_raw):\n",
    "            continue\n",
    "        \n",
    "        if isinstance(pubs_raw, list):\n",
    "            pub_list = [str(p).strip() for p in pubs_raw if str(p).strip()]\n",
    "        else:\n",
    "            pub_list = [p.strip() for p in str(pubs_raw).split(\",\") if p.strip()]\n",
    "\n",
    "        if not pub_list:\n",
    "            continue\n",
    "        \n",
    "        combined_pubs = \", \".join(pub_list)\n",
    "        rows.append({\"appid\": appid, \"publisher_name\": combined_pubs})\n",
    "\n",
    "    df_flat = pd.DataFrame(rows).reset_index(drop=True)\n",
    "\n",
    "    df_flat.insert(1, \"pubid\", range(1, len(df_flat)+1))\n",
    "\n",
    "    game_publisher_df = df_flat[['appid', 'pubid']].copy()\n",
    "    publishers_df = df_flat[['pubid', 'publisher_name']].copy()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        publishers_path = os.path.join(output_dir, \"publishers.csv\")\n",
    "        game_publisher_path = os.path.join(output_dir, \"game_publisher.csv\")\n",
    "        publishers_df.to_csv(publishers_path, index=False)\n",
    "        game_publisher_df.to_csv(game_publisher_path, index=False)\n",
    "        logging.info(f\"Saved 'publishers.csv' ({len(publishers_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'game_publisher.csv' ({len(game_publisher_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return publishers_df, game_publisher_df\n",
    "\n",
    "def create_categories_table(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Létrehozza a game_category és categories táblákat:\n",
    "    - game_category: appid + catid\n",
    "    - categories: catid + name (eredeti categories mező)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        categories_raw = row.get(\"categories\", \"\")\n",
    "\n",
    "        text = str(categories_raw).strip()\n",
    "        if text in [\"\", \"[]\", \"['']\"]:\n",
    "            continue\n",
    "\n",
    "        rows.append({\"appid\": appid, \"name\": text})\n",
    "\n",
    "    df_flat = pd.DataFrame(rows).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    df_flat.insert(1, \"catid\", range(1, len(df_flat)+1))\n",
    "\n",
    "    game_category_df = df_flat[['appid', 'catid']].copy()\n",
    "    categories_df = df_flat[['catid', 'name']].copy()\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        categories_path = os.path.join(output_dir, \"categories.csv\")\n",
    "        game_category_path = os.path.join(output_dir, \"game_category.csv\")\n",
    "        categories_df.to_csv(categories_path, index=False)\n",
    "        game_category_df.to_csv(game_category_path, index=False)\n",
    "        logging.info(f\"Saved 'categories.csv' ({len(categories_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'game_category.csv' ({len(game_category_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return categories_df, game_category_df\n",
    "\n",
    "\n",
    "def create_tags_table(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    Egyszerűsített tags táblageneráló függvény.\n",
    "    A merged_master.csv 'tags' oszlopából két táblát készít:\n",
    "      - game_tag.csv: appid–tagid kapcsolatok\n",
    "      - tags.csv: tagid–tag_name–weight lista\n",
    "    \"\"\"\n",
    "    rows_game_tag = []\n",
    "    rows_tags = []\n",
    "    tagid_counter = 1\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "        tags_val = row.get(\"tags\")\n",
    "\n",
    "        if not tags_val or pd.isna(tags_val):\n",
    "            continue\n",
    "\n",
    "        if isinstance(tags_val, str):\n",
    "            try:\n",
    "                tags_val = ast.literal_eval(tags_val)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if not isinstance(tags_val, list):\n",
    "            continue\n",
    "\n",
    "        for tag_entry in tags_val:\n",
    "            if not isinstance(tag_entry, dict):\n",
    "                continue\n",
    "            tag_name = tag_entry.get(\"tag_name\")\n",
    "            weight = tag_entry.get(\"weight\", 1)\n",
    "\n",
    "            if not tag_name:\n",
    "                continue\n",
    "\n",
    "            rows_game_tag.append({\"appid\": appid, \"tagid\": tagid_counter})\n",
    "            rows_tags.append({\"tagid\": tagid_counter, \"tag_name\": tag_name, \"weight\": weight})\n",
    "            tagid_counter += 1\n",
    "\n",
    "    game_tag_df = pd.DataFrame(rows_game_tag)\n",
    "    tags_df = pd.DataFrame(rows_tags)\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        game_tag_path = os.path.join(output_dir, \"game_tag.csv\")\n",
    "        tags_path = os.path.join(output_dir, \"tags.csv\")\n",
    "\n",
    "        game_tag_df.to_csv(game_tag_path, index=False, encoding=\"utf-8-sig\")\n",
    "        tags_df.to_csv(tags_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        logging.info(f\"Saved 'game_tag.csv' ({len(game_tag_df)} rows) to {output_dir}\")\n",
    "        logging.info(f\"Saved 'tags.csv' ({len(tags_df)} rows) to {output_dir}\")\n",
    "\n",
    "    return game_tag_df, tags_df\n",
    "\n",
    "def create_languages_table(master_df: pd.DataFrame, output_dir: str = None):\n",
    "    \"\"\"\n",
    "    A master_df 'supported_languages' és 'full_audio_languages' oszlopaiból\n",
    "    három táblát hoz létre a relációs séma szerint:\n",
    "\n",
    "    1. languages.csv             → id | name\n",
    "    2. game_subtitles.csv        → appid | languageid  (feliratok)\n",
    "    3. game_audio_language.csv   → appid | languageid  (hang)\n",
    "    \"\"\"\n",
    "    lang_name_to_id = {}\n",
    "    next_lang_id = 1\n",
    "\n",
    "    rows_languages = []\n",
    "    rows_game_subtitles = []\n",
    "    rows_game_audio = []\n",
    "\n",
    "    for _, row in master_df.iterrows():\n",
    "        appid = row[\"appid\"]\n",
    "\n",
    "        supported_raw = row.get(\"supported_languages\", \"\")\n",
    "        supported = []\n",
    "        if pd.notna(supported_raw) and str(supported_raw).strip():\n",
    "            try:\n",
    "                val = ast.literal_eval(str(supported_raw))\n",
    "                if isinstance(val, list):\n",
    "                    supported = [v.strip() for v in val if isinstance(v, str) and v.strip()]\n",
    "                elif isinstance(val, str):\n",
    "                    supported = [v.strip() for v in val.split(\",\") if v.strip()]\n",
    "            except Exception:\n",
    "                supported = [v.strip() for v in str(supported_raw).split(\",\") if v.strip()]\n",
    "\n",
    "        audio_raw = row.get(\"full_audio_languages\", \"\")\n",
    "        full_audio = []\n",
    "        if pd.notna(audio_raw) and str(audio_raw).strip():\n",
    "            try:\n",
    "                val = ast.literal_eval(str(audio_raw))\n",
    "                if isinstance(val, list):\n",
    "                    full_audio = [v.strip() for v in val if isinstance(v, str) and v.strip()]\n",
    "                elif isinstance(val, str):\n",
    "                    full_audio = [v.strip() for v in val.split(\",\") if v.strip()]\n",
    "            except Exception:\n",
    "                full_audio = [v.strip() for v in str(audio_raw).split(\",\") if v.strip()]\n",
    "\n",
    "        all_langs = set(supported + full_audio)\n",
    "\n",
    "        for lang in all_langs:\n",
    "            if lang not in lang_name_to_id:\n",
    "                lang_name_to_id[lang] = next_lang_id\n",
    "                rows_languages.append({\"id\": next_lang_id, \"name\": lang})\n",
    "                next_lang_id += 1\n",
    "\n",
    "            langid = lang_name_to_id[lang]\n",
    "\n",
    "            if lang in supported:\n",
    "                rows_game_subtitles.append({\"appid\": appid, \"languageid\": langid})\n",
    "            if lang in full_audio:\n",
    "                rows_game_audio.append({\"appid\": appid, \"languageid\": langid})\n",
    "\n",
    "    languages_df = pd.DataFrame(rows_languages)\n",
    "    game_subtitles_df = pd.DataFrame(rows_game_subtitles)\n",
    "    game_audio_df = pd.DataFrame(rows_game_audio)\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        languages_df.to_csv(os.path.join(output_dir, \"languages.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        game_subtitles_df.to_csv(os.path.join(output_dir, \"game_subtitles.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "        game_audio_df.to_csv(os.path.join(output_dir, \"game_audio_language.csv\"), index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        logging.info(f\"Saved languages ({len(languages_df)}), \"\n",
    "                     f\"game_subtitles ({len(game_subtitles_df)}), \"\n",
    "                     f\"game_audio_language ({len(game_audio_df)}) to {output_dir}\")\n",
    "\n",
    "    return languages_df, game_subtitles_df, game_audio_df\n",
    "\n",
    "\n",
    "def create_description_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a 'description' táblát a master DataFrame-ből.\n",
    "\n",
    "    Tartalmazza:\n",
    "      - descriptionid (1-től generált)\n",
    "      - appid\n",
    "      - detailed_description\n",
    "      - about_the_game\n",
    "      - short_description\n",
    "    \"\"\"\n",
    "    cols = [\"appid\", \"detailed_description\", \"about_the_game\", \"short_description\"]\n",
    "    existing_cols = [c for c in cols if c in master_df.columns]\n",
    "\n",
    "    if not existing_cols:\n",
    "        logging.warning(\"No description columns found in master dataframe.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = master_df[existing_cols].copy()\n",
    "\n",
    "    df = df[\n",
    "        (df.get(\"detailed_description\", \"\") != \"\") |\n",
    "        (df.get(\"about_the_game\", \"\") != \"\") |\n",
    "        (df.get(\"short_description\", \"\") != \"\")\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    df.insert(0, \"descriptionid\", range(1, len(df) + 1))\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"description.csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "        logging.info(f\"Saved 'description.csv' ({len(df)} rows) to {output_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def create_game_table(master_df: pd.DataFrame, output_dir: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Létrehozza a 'game.csv' táblát a master DataFrame-ből.\n",
    "    Csak az appid, name és release_date mezőket tartalmazza.\n",
    "    Nem szűri ki az üres neveket.\n",
    "    \"\"\"\n",
    "    cols = [\"appid\", \"name\", \"release_date\",\"estimated_owners\",\"required_age\",\"price\",\"dlc_count\",\"recommendations\",\"notes\",\n",
    "           \"website\",\"metacritic_score\",\"metacritic_url\",\"achievements\",\"user_score\",\"score_rank\",\"positive\",\"negative\",\n",
    "           \"average_playtime_forever\",\"average_playtime_2weeks\",\"median_playtime_forever\",\"median_playtime_2weeks\",\n",
    "           \"peak_ccu\",\"discount\",\"pct_pos_total\",\"pct_pos_recent\",\"num_reviews_total\",\"num_reviews_recent\",\"reviews\",\"english\"]\n",
    "    existing_cols = [c for c in cols if c in master_df.columns]\n",
    "\n",
    "    if \"appid\" not in existing_cols:\n",
    "        logging.warning(\"Missing 'appid' column in master dataframe.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = master_df[existing_cols].copy()\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    if \"recommendations\" in df.columns:\n",
    "        df.rename(columns={\"recommendations\": \"num_recommendations\"}, inplace=True)\n",
    "\n",
    "    if \"achievements\" in df.columns:\n",
    "        df.rename(columns={\"achievements\": \"num_achievements\"}, inplace=True)\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        path = os.path.join(output_dir, \"game.csv\")\n",
    "        df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "        logging.info(f\"Saved 'game.csv' ({len(df)} rows) to {output_dir}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    logging.info(\"=== Starting splitting process ===\")\n",
    "    D = load_csv_safely(os.path.join(D_PATH, \"merged_master.csv\"))\n",
    "    \n",
    "    media_df = create_media_table(D, output_dir=OUTPUT_PATH)\n",
    "    screenshot_df = create_screenshots_table(D, output_dir=OUTPUT_PATH)\n",
    "    movies_df = create_movies_table(D, output_dir=OUTPUT_PATH)\n",
    "    support_df = create_support_table(D, output_dir=OUTPUT_PATH)\n",
    "    requirements_df = create_requirements_table(D, output_dir=OUTPUT_PATH)\n",
    "    platforms_df = create_platforms_table(D, output_dir=OUTPUT_PATH)\n",
    "    packages_df = create_packages_table(D, output_dir=OUTPUT_PATH)\n",
    "    developer_df = create_developer_tables(D, output_dir=OUTPUT_PATH)\n",
    "    publisher_df = create_publisher_tables(D, output_dir=OUTPUT_PATH)\n",
    "    genres_df = create_genres_table(D, output_dir=OUTPUT_PATH)\n",
    "    categories_df = create_categories_table(D, output_dir=OUTPUT_PATH)\n",
    "    tags_df = create_tags_table(D, output_dir=OUTPUT_PATH)\n",
    "    description_df = create_description_table(D, output_dir=OUTPUT_PATH)\n",
    "    game_df = create_game_table(D, output_dir=OUTPUT_PATH)\n",
    "    languages_df = create_languages_table(D, output_dir=OUTPUT_PATH)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGS columns: ['id', 'name']\n",
      "SUBS columns: ['appid', 'languageid']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "langs = pd.read_csv(r\"C:\\Users\\zalma\\split\\languages.csv\")\n",
    "subs = pd.read_csv(r\"C:\\Users\\zalma\\split\\game_subtitles.csv\")\n",
    "\n",
    "print(\"LANGS columns:\", langs.columns.tolist())\n",
    "print(\"SUBS columns:\", subs.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       appid                     name\n",
      "3725  292030  Spanish - Latin America\n",
      "3726  292030                   Polish\n",
      "3727  292030       Simplified Chinese\n",
      "3728  292030                   Korean\n",
      "3729  292030      Portuguese - Brazil\n",
      "3730  292030                  Russian\n",
      "3731  292030                  English\n",
      "3732  292030                   French\n",
      "3733  292030                 Japanese\n",
      "3734  292030                   German\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "langs = pd.read_csv(r\"C:\\Users\\zalma\\split\\languages.csv\")\n",
    "subs = pd.read_csv(r\"C:\\Users\\zalma\\split\\game_audio_language.csv\")\n",
    "\n",
    "appid = 292030\n",
    "\n",
    "query = (\n",
    "    subs.merge(langs, left_on=\"languageid\", right_on=\"id\", how=\"left\")\n",
    "         .query(\"appid == @appid\")[[\"appid\", \"name\"]]\n",
    ")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged = pd.read_csv(r\"C:\\Users\\zalma\\merge\\merged_master.csv\", dtype=str)\n",
    "game = pd.read_csv(r\"C:\\Users\\zalma\\split\\game.csv\", dtype=str)\n",
    "\n",
    "missing = merged[~merged[\"appid\"].isin(game[\"appid\"])]\n",
    "print(f\"Hiányzó sorok száma: {len(missing)}\")\n",
    "display(missing[[\"appid\", \"name\", \"release_date\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/media.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/screenshots.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/movies.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/support.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/requirements.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_platform.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/platforms.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_package.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/packages.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/sub_package.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_developer.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/developers.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_publisher.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/publishers.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_genre.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/genres.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_category.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/categories.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/game_tag.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = 'C:/Users/zalma/split/tags.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"CSV loaded successfully!\")\n",
    "    display(df.head(10))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at {csv_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6q1IaRT88/NmyZgKb44yw",
   "mount_file_id": "1-2PN27SNYh5xNK4Ec8WdXpfCSqfK9dOs",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
